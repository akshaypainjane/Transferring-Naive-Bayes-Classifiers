{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b01a13",
      "metadata": {
        "id": "84b01a13"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import math\n",
        "\n",
        "from sklearn import model_selection\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ffe419",
      "metadata": {
        "id": "40ffe419"
      },
      "outputs": [],
      "source": [
        "X_train = [] # an element of X is represented as (filename,text)\n",
        "Y_train = [] # an element of Y represents the newsgroup category of the corresponding X element\n",
        "for category in os.listdir('train'):\n",
        "    for document in os.listdir('train/'+category):\n",
        "        with open('train/'+category+'/'+document, \"r\") as f:\n",
        "            X_train.append((document,f.read()))\n",
        "            Y_train.append(category)\n",
        "            \n",
        "num_category = 2 #Our task is a 2 category classification task           "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba745b79",
      "metadata": {
        "id": "ba745b79"
      },
      "source": [
        "### Train has 0: rec.autos and 1: talk.politics.guns \n",
        "### Test has 0: rec.motorcycles and 1: talk.politics.misc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "942c3855",
      "metadata": {
        "id": "942c3855"
      },
      "source": [
        "## Category mapping is 0: Rec and 1:Talk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7136dacb",
      "metadata": {
        "id": "7136dacb"
      },
      "outputs": [],
      "source": [
        "Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3be60f7",
      "metadata": {
        "id": "a3be60f7"
      },
      "outputs": [],
      "source": [
        "X_test = [] # an element of X is represented as (filename,text)\n",
        "Y_test = [] # an element of Y represents the newsgroup category of the corresponding X element\n",
        "for category in os.listdir('test'):\n",
        "    for document in os.listdir('test/'+category):\n",
        "        with open('test/'+category+'/'+document, \"r\") as f:\n",
        "            X_test.append((document,f.read()))\n",
        "            Y_test.append(category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3a61712",
      "metadata": {
        "id": "f3a61712"
      },
      "outputs": [],
      "source": [
        "Y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2baf582d",
      "metadata": {
        "id": "2baf582d"
      },
      "outputs": [],
      "source": [
        "# A list of common english words which should not affect predictions\n",
        "stopwords = ['those', 'on', 'own', 'yourselves', 'ie', 'around', 'between', 'four', 'been', 'alone', 'off', 'am', 'then', 'other',\n",
        "             'can', 'cry', 'hereafter', 'front', 'too', 'wherein', 'everything', 'up', 'onto', 'never', 'either', 'how', 'before', \n",
        "             'anyway', 'since', 'through', 'amount', 'now', 'he', 'cant', 'was', 'con', 'have', 'into', 'because', 'inc', 'not', \n",
        "             'therefore', 'they', 'even', 'whom', 'it', 'see', 'somewhere', 'interest', 'thereupon', 'nothing', 'thick', 'whereas', \n",
        "             'much', 'whenever', 'find', 'seem', 'until', 'whereby', 'at', 'ltd', 'fire', 'also', 'some', 'last', 'than', 'get', \n",
        "             'already', 'our', 'once', 'will', 'noone', 'that', 'what', 'thus', 'no', 'myself', 'out', 'next', 'whatever', 'although', \n",
        "             'though', 'etc', 'which', 'would', 'therein', 'nor', 'somehow', 'whereupon', 'besides', 'whoever', 'thin', 'ourselves', \n",
        "             'few', 'third', 'without', 'anything', 'twelve', 'against', 'while', 'twenty', 'if', 'however', 'found', 'herself', \n",
        "             'when', 'may', 'ours', 'six', 'done', 'seems', 'else', 'call', 'perhaps', 'had', 'nevertheless', 'fill', 'where', \n",
        "             'otherwise', 'still', 'within', 'its', 'for', 'together', 'elsewhere', 'throughout', 'of', 'eg', 'others', 'show', \n",
        "             'sincere', 'anywhere', 'anyhow', 'as', 'are', 'the', 'hence', 'something', 'hereby', 'nowhere', 'de', 'latterly', \n",
        "             'neither', 'his', 'go', 'forty', 'put', 'their', 'by', 'namely', 'could', 'five', 'itself', 'is', 'nine', 'whereafter', \n",
        "             'down', 'bottom', 'thereby', 'such', 'both', 'she', 'become', 'whole', 'who', 'yourself', 'every', 'thru', 'except', \n",
        "             'very', 'several', 'among', 'being', 'be', 'mine', 'further', 'here', 'during', 'why', 'with', 'becomes', 'about', \n",
        "             'a', 'co', 'seeming', 'due', 'wherever', 'beforehand', 'detail', 'fifty', 'becoming', 'might', 'amongst', 'my', 'empty', \n",
        "             'thence', 'thereafter', 'almost', 'least', 'someone', 'often', 'from', 'keep', 'him', 'or', 'top', 'her', 'nobody',\n",
        "             'sometime', 'across', 'hundred', 'only', 'via', 'name', 'eight', 'three', 'back', 'to', 'all', 'became', 'move', 'me', \n",
        "             'we', 'formerly', 'so', 'i', 'whence', 'describe', 'under', 'always', 'himself', 'in', 'herein', 'more', 'after', \n",
        "             'themselves', 'you', 'above', 'sixty', 'them', 'hasnt', 'your', 'made', 'indeed', 'most', 'everywhere', 'fifteen', \n",
        "             'but', 'must', 'along', 'beside', 'hers', 'side', 'former', 'anyone', 'full', 'has', 'yours', 'whose', 'behind', \n",
        "             'please', 'amoungst', 'mill', 'ten', 'seemed', 'sometimes', 'should', 'over', 'take', 'each', 'same', 'rather', 'latter',\n",
        "             'and', 'hereupon', 'part', 'per', 'eleven', 'ever', 'enough', 'again', 'us', 'yet', 'moreover', 'mostly', 'one', 'meanwhile',\n",
        "             'whither', 'there', 'toward', 'give', 'system', 'do', 'an', 'these', 'everyone', 'towards', 'this', 'bill', 'cannot', 'un', \n",
        "             'afterwards', 'beyond', 'were', 'whether', 'well', 'another', 'below', 'first', 'upon', 'any', 'none', 'many', 'serious', \n",
        "             're', 'two', 'couldnt', 'less''a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost','alone',\n",
        "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
        "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
        "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
        "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
        "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
        "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
        "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
        "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
        "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
        "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
        "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
        "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
        "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
        "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
        "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
        "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
        "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
        "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
        "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
        "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
        "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
        "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
        "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
        "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
        "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
        "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
        "             'yourselves']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de301211",
      "metadata": {
        "id": "de301211"
      },
      "source": [
        "## Building a vocabulary of words from the given documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2141cf64",
      "metadata": {
        "id": "2141cf64"
      },
      "outputs": [],
      "source": [
        "vocab = {}    #dictionary with unique words (key) and their freq (value)\n",
        "for i in range(len(X_train)):   #ith document\n",
        "    word_list = []\n",
        "    for word in X_train[i][1].split():   #X_train[i][0] has file no.\n",
        "        word_new  = word.strip(string.punctuation).lower()   #strip(..) removes punctuation characters from beginning and end\n",
        "        if (len(word_new)>2)  and (word_new not in stopwords):  \n",
        "            if word_new in vocab:\n",
        "                vocab[word_new]+=1\n",
        "            else:\n",
        "                vocab[word_new]=1            "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16af9a56",
      "metadata": {
        "id": "16af9a56"
      },
      "source": [
        "### Plotting a graph of no of words with a given frequency to decide cutoff drequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b98faa9",
      "metadata": {
        "id": "0b98faa9"
      },
      "outputs": [],
      "source": [
        "num_words = [0 for i in range(max(vocab.values())+1)]  # i goes till it covers all frequenicies till most freq word, num_words is a list of all possible freq\n",
        "freq = [i for i in range(max(vocab.values())+1)]       # x axis\n",
        "total_words = 0\n",
        "\n",
        "for key in vocab:\n",
        "    num_words[vocab[key]]+=1  # num_words[with this freq] = ? how much\n",
        "    \n",
        "for i in range (len(num_words)):\n",
        "    total_words += num_words[i]\n",
        "plt.plot(freq,num_words)\n",
        "plt.axis([1, 10, 0, 20000])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"No of words\")  #no of words with each freq\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a205cb3",
      "metadata": {
        "id": "8a205cb3"
      },
      "outputs": [],
      "source": [
        "cutoff_freq = 150\n",
        "# For deciding cutoff frequency\n",
        "num_words_above_cutoff = len(vocab)-sum(num_words[0:cutoff_freq]) \n",
        "print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),num_words_above_cutoff)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f47772",
      "metadata": {
        "id": "09f47772"
      },
      "source": [
        "### Words with frequency higher than cutoff frequency are chosen as features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94828662",
      "metadata": {
        "id": "94828662"
      },
      "outputs": [],
      "source": [
        "# (i.e we remove words with low frequencies as they would not be significant )\n",
        "features = []\n",
        "for key in vocab:\n",
        "    if vocab[key] >=cutoff_freq:\n",
        "        features.append(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "647fe9f7",
      "metadata": {
        "id": "647fe9f7"
      },
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ad9f2e",
      "metadata": {
        "id": "65ad9f2e"
      },
      "outputs": [],
      "source": [
        "# To represent training data as word vector counts\n",
        "X_train_dataset = np.zeros((len(X_train),len(features)))\n",
        "# This can take some time to complete\n",
        "for i in range(len(X_train)):\n",
        "#     print(i) # Uncomment to see progress\n",
        "    word_list = [ word.strip(string.punctuation).lower() for word in X_train[i][1].split()]\n",
        "    for word in word_list:\n",
        "        if word in features:\n",
        "            X_train_dataset[i][features.index(word)] += 1   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc2e5df",
      "metadata": {
        "id": "edc2e5df"
      },
      "outputs": [],
      "source": [
        "X_train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac68150",
      "metadata": {
        "id": "9ac68150"
      },
      "outputs": [],
      "source": [
        "# To represent test data as word vector counts\n",
        "X_test_dataset = np.zeros((len(X_test),len(features)))\n",
        "# This can take some time to complete\n",
        "for i in range(len(X_test)):\n",
        "    # print(i) # Uncomment to see progress\n",
        "    word_list = [ word.strip(string.punctuation).lower() for word in X_test[i][1].split()]\n",
        "    for word in word_list:\n",
        "        if word in features:\n",
        "            X_test_dataset[i][features.index(word)] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf7bf58",
      "metadata": {
        "scrolled": true,
        "id": "ccf7bf58"
      },
      "outputs": [],
      "source": [
        "X_test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac91fe0",
      "metadata": {
        "id": "fac91fe0"
      },
      "outputs": [],
      "source": [
        "Y_train_dataset = np.zeros(len(Y_train))\n",
        "for i in range(len(Y_train)):\n",
        "    if(Y_train[i].find(\"talk\")):\n",
        "        Y_train_dataset[i] = 1\n",
        "        \n",
        "Y_test_dataset = np.zeros(len(Y_test))\n",
        "for i in range(len(Y_test)):\n",
        "    if(Y_test[i].find(\"talk\")):\n",
        "        Y_test_dataset[i] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda6aefd",
      "metadata": {
        "id": "fda6aefd"
      },
      "source": [
        "### Evaluating on Traditional Naive Bayes Classifier Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8019f08",
      "metadata": {
        "id": "c8019f08"
      },
      "outputs": [],
      "source": [
        "# Implementing Multinomial Naive Bayes from scratch\n",
        "class MultinomialNaiveBayes:\n",
        "    \n",
        "    def __init__(self):\n",
        "        # count is a dictionary which stores several dictionaries corresponding to each news category\n",
        "        # each value in the subdictionary represents the freq of the key corresponding to that news category \n",
        "        self.count = {}\n",
        "        # classes represents the different news categories\n",
        "        self.classes = None\n",
        "    \n",
        "    def fit(self,X_train,Y_train):\n",
        "        # This can take some time to complete       \n",
        "        self.classes = set(Y_train)\n",
        "        for class_ in self.classes:\n",
        "            self.count[class_] = {}\n",
        "            for i in range(len(X_train[0])):\n",
        "                self.count[class_][i] = 0\n",
        "            self.count[class_]['total'] = 0\n",
        "            self.count[class_]['total_points'] = 0\n",
        "        self.count['total_points'] = len(X_train)\n",
        "        \n",
        "        for i in range(len(X_train)):\n",
        "            for j in range(len(X_train[0])):\n",
        "                self.count[Y_train[i]][j]+=X_train[i][j]\n",
        "                self.count[Y_train[i]]['total']+=X_train[i][j]\n",
        "            self.count[Y_train[i]]['total_points']+=1\n",
        "    \n",
        "    def __probability(self,test_point,class_):\n",
        "        \n",
        "        log_prob = np.log(self.count[class_]['total_points']) - np.log(self.count['total_points'])\n",
        "        total_words = len(test_point)\n",
        "        for i in range(len(test_point)):\n",
        "            current_word_prob = test_point[i]*(np.log(self.count[class_][i]+1)-np.log(self.count[class_]['total']+total_words))\n",
        "            log_prob += current_word_prob\n",
        "        \n",
        "        return log_prob\n",
        "    \n",
        "    \n",
        "    def __predictSinglePoint(self,test_point):\n",
        "        \n",
        "        best_class = None\n",
        "        best_prob = None\n",
        "        first_run = True\n",
        "        \n",
        "        for class_ in self.classes:\n",
        "            log_probability_current_class = self.__probability(test_point,class_)\n",
        "            if (first_run) or (log_probability_current_class > best_prob) :\n",
        "                best_class = class_\n",
        "                best_prob = log_probability_current_class\n",
        "                first_run = False\n",
        "                \n",
        "        return best_class\n",
        "        \n",
        "  \n",
        "    def predict(self,X_test):\n",
        "        # This can take some time to complete\n",
        "        Y_pred = [] \n",
        "        for i in range(len(X_test)):\n",
        "        # print(i) # Uncomment to see progress\n",
        "            Y_pred.append( self.__predictSinglePoint(X_test[i]) )\n",
        "        \n",
        "        return Y_pred\n",
        "    \n",
        "    def score(self,Y_pred,Y_true):\n",
        "        # returns the mean accuracy\n",
        "        count = 0\n",
        "        for i in range(len(Y_pred)):\n",
        "            if Y_pred[i] == Y_true[i]:\n",
        "                count+=1\n",
        "        return count/len(Y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15afe9d7",
      "metadata": {
        "scrolled": false,
        "id": "15afe9d7"
      },
      "outputs": [],
      "source": [
        "clf2 = MultinomialNaiveBayes()\n",
        "clf2.fit(X_train_dataset,Y_train)\n",
        "Y_test_pred = clf2.predict(X_test_dataset)\n",
        "our_score_test = clf2.score(Y_test_pred,Y_test)  \n",
        "print(\"Our score on testing data :\",our_score_test)\n",
        "print(\"Classification report for testing data :-\")\n",
        "print(classification_report(Y_test, Y_test_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c452fba8",
      "metadata": {
        "id": "c452fba8"
      },
      "outputs": [],
      "source": [
        "Y_test_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161ccb9f",
      "metadata": {
        "id": "161ccb9f"
      },
      "outputs": [],
      "source": [
        "Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6774c0d",
      "metadata": {
        "id": "d6774c0d"
      },
      "source": [
        "### Evaluating on the proposed NBTC algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289e2242",
      "metadata": {
        "id": "289e2242"
      },
      "outputs": [],
      "source": [
        "#Defining KL divergence between two numpy arrays\n",
        "def KL(a, b):\n",
        "    a = np.asarray(a, dtype=np.float)\n",
        "    b = np.asarray(b, dtype=np.float)\n",
        "    return np.sum(np.where(b!=0, a * np.log(a / b), 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bf07057",
      "metadata": {
        "id": "6bf07057"
      },
      "outputs": [],
      "source": [
        "X_train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a344c72d",
      "metadata": {
        "id": "a344c72d"
      },
      "outputs": [],
      "source": [
        "p1 = []\n",
        "tot_sum,count = 0,0\n",
        "for i in range(len(X_train_dataset[0])):\n",
        "    count = 0\n",
        "    for j in range(len(X_train_dataset)):\n",
        "        count += X_train_dataset[j][i]\n",
        "    p1.append(count)\n",
        "         \n",
        "for i in range(len(p1)):\n",
        "    tot_sum += p1[i]\n",
        "\n",
        "for i in range(len(p1)):\n",
        "    p1[i] = p1[i]/tot_sum\n",
        "        \n",
        "print(p1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a90176",
      "metadata": {
        "id": "98a90176"
      },
      "outputs": [],
      "source": [
        "p2 = []\n",
        "tot_sum2,count2 = 0,0\n",
        "for i in range(len(X_test_dataset[0])):\n",
        "    count2 = 0\n",
        "    for j in range(len(X_test_dataset)):\n",
        "        count2 += X_test_dataset[j][i]\n",
        "    p2.append(count2)\n",
        "         \n",
        "for i in range(len(p2)):\n",
        "    tot_sum2 += p2[i]\n",
        "\n",
        "for i in range(len(p2)):\n",
        "    p2[i] = p2[i]/tot_sum2\n",
        "        \n",
        "print(p2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02a8691",
      "metadata": {
        "id": "b02a8691"
      },
      "outputs": [],
      "source": [
        "kl_div = KL(p1, p2)\n",
        "print(kl_div)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5815f084",
      "metadata": {
        "id": "5815f084"
      },
      "source": [
        "The KL divergence between train and test data is around 0.315"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abe76998",
      "metadata": {
        "id": "abe76998"
      },
      "outputs": [],
      "source": [
        "#Using the empirical relation between KL divergence and gamma to find gamma value for a given KL divergence value\n",
        "gamma = 0.042*(math.pow(kl_div, -2.276))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f918c056",
      "metadata": {
        "id": "f918c056"
      },
      "outputs": [],
      "source": [
        "gamma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b12b80f",
      "metadata": {
        "id": "2b12b80f"
      },
      "outputs": [],
      "source": [
        "PDu_Dl_ = gamma/(1+gamma)\n",
        "PDu_Du_ = 1 - PDu_Dl_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca0915f",
      "metadata": {
        "id": "9ca0915f"
      },
      "outputs": [],
      "source": [
        "PDu_Dl_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef43f170",
      "metadata": {
        "id": "ef43f170"
      },
      "source": [
        "### Algorithm 1 as given in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f92735d",
      "metadata": {
        "id": "0f92735d"
      },
      "outputs": [],
      "source": [
        "def Algo1():\n",
        "    # Every element is ith iteration of this\n",
        "    # Every element has C sub_elements for each class 'c'\n",
        "    # Every sub_element has sub_sub_element for every document 'd'\n",
        "    \n",
        "    c1 = 0\n",
        "    for i in Y_train_dataset:\n",
        "        if(i == 1):\n",
        "            c1+=1\n",
        "\n",
        "    p1 = c1/len(Y_train_dataset)\n",
        "    p0 = 1-p1\n",
        "    \n",
        "    T = 10  # Number of times we run the loop, more is T, more accurate will be the model\n",
        "    PDu_cId =[[[0 for j in range(2)] for k in range(len(X_train_dataset))] for i in range(T+1)] # 0th iteration value 0 (lets say)\n",
        "    PDu_wIc = [[[0 for k in range(len(features))] for j in range(2)] for i in range(T+1)] # ith iteration, jth class, kth word\n",
        "    PDu_c = [[0 for j in range(2)] for i in range(T+1)]   # ith iteration, jth class\n",
        "    PDu_d = [[1/len(X_train_dataset) for j in range(len(X_train_dataset))] for i in range(T+1)]   # ith iteration, jth document\n",
        "    \n",
        "    PDu_cIDu = [[0 for j in range(2)] for i in range(T+1)]\n",
        "    PDu_cIDl = [[0 for j in range(2)] for i in range(T+1)]\n",
        "    nDu_w_c_Dl = [[0 for i in range(len(features))] for j in range(2)]\n",
        "    nDu_w_c_Du = [[0 for i in range(len(features))] for j in range(2)]\n",
        "    nDu_c_Dl = [0 for i in range(2)]\n",
        "    nDu_c_Du = [0 for i in range(2)]\n",
        "    \n",
        "    PDu_wIc_Dl = [[0 for j in range(2)] for i in range(len(features))]\n",
        "    PDu_wIc_Du = [[0 for j in range(2)] for i in range(len(features))]\n",
        "    \n",
        "    PDu_Du = []\n",
        "    \n",
        "    PDu_Dl = []\n",
        "    #initializing for 0th Iteration using traditional naive bayes classifier\n",
        "    PDu_c[0][0] = p0\n",
        "    PDu_c[0][1] = p1\n",
        "    \n",
        "    \n",
        "    nDu_w_c = [[0 for i in range(len(features))] for j in range(2)]\n",
        "    nC = [0 for i in range(2)]\n",
        "    for w in range(len(features)):\n",
        "        for c in range(num_category):\n",
        "            for d in range(len(X_train_dataset)):  #We estimate PDu(.) using PDl(.) by traditional naive bayes\n",
        "                if(Y_train_dataset[d] == c):\n",
        "                    nDu_w_c[c][w] += X_train_dataset[d][w]\n",
        "                    nC[c] += 1\n",
        "    \n",
        "#     print(nDu_w_c)\n",
        "#     print(nC)\n",
        "    \n",
        "    for c in range(num_category):\n",
        "        for w in range(len(features)):\n",
        "            PDu_wIc[0][c][w] = (1+nDu_w_c[c][w])/(len(features)+nC[c])\n",
        "\n",
        "#     print(PDu_wIc[0])\n",
        "#     print(PDu_c[0])\n",
        "#     print(PDu_d)\n",
        "\n",
        "    for t in range(1,T+1):\n",
        "        # Start for Eqn 6\n",
        "        print(t)\n",
        "        for c in range(num_category):\n",
        "            for d in range(len(X_test_dataset)):\n",
        "                val1 = PDu_c[t-1][c]\n",
        "                for w in range(len(X_test_dataset[d])):\n",
        "                    if(X_test_dataset[d][w] > 0 and Y_test_dataset[d] == c): \n",
        "                        val1 *= PDu_wIc[t-1][c][w]\n",
        "                PDu_cId[t][d][c] = val1\n",
        "                \n",
        "        # End for Eqn 6\n",
        "        for c in range(num_category):\n",
        "            # Start for Eqn 7\n",
        "            #Eqn 9 start               \n",
        "            for d in range (len(X_train_dataset)):\n",
        "                PDu_cIDu[t][c] += PDu_cId[t][d][c]*(1/len(X_train_dataset))  #doubt\n",
        "\n",
        "            for d in range (len(X_test_dataset)):\n",
        "                PDu_cIDl[t][c] += PDu_cId[t][d][c]*(1/len(X_test_dataset))   #doubt\n",
        "            #Eqn 9 end\n",
        "\n",
        "            PDu_c[t][c] = PDu_Du_*PDu_cIDu[t][c] + PDu_Dl_*PDu_cIDl[t][c]   #PDu_Du_ is tot prob for all classes c's\n",
        "            # End for Eqn 7\n",
        "\n",
        "            #Eqn 11 start\n",
        "            for w in range(len(features)):\n",
        "                for c in range(num_category):\n",
        "                    for d in range(len(X_train_dataset)):\n",
        "                        nDu_w_c_Dl[c][w] += np.sum(X_train_dataset[d])*(X_train_dataset[d][w]/np.sum(X_train_dataset[d]))*PDu_cId[t][d][c]\n",
        "                    for d in range(len(X_test_dataset)):\n",
        "                        nDu_w_c_Du[c][w] += np.sum(X_test_dataset[d])*(X_test_dataset[d][w]/np.sum(X_test_dataset[d]))*PDu_cId[t][d][c]\n",
        "            #Eqn 11 end\n",
        "\n",
        "\n",
        "            #Eqn 12 start\n",
        "            for d in range (len(X_train_dataset)):\n",
        "                nDu_c_Dl[c] += np.sum(X_train_dataset[d])*PDu_cId[t][d][c]\n",
        "\n",
        "            for d in range (len(X_test_dataset)):\n",
        "                nDu_c_Du[c] += np.sum(X_train_dataset[d])*PDu_cId[t][d][c]\n",
        "            #Eqn 12 end\n",
        "\n",
        "            #Eqn 10 start\n",
        "            for w in range(len(features)):\n",
        "                for c in range(num_category):\n",
        "                    PDu_wIc_Dl[w][c] = (1+nDu_w_c_Dl[c][w])/(len(features)+nDu_c_Dl[c])\n",
        "                    PDu_wIc_Du[w][c] = (1+nDu_w_c_Du[c][w])/(len(features)+nDu_c_Du[c])\n",
        "            #Eqn 10 end\n",
        "\n",
        "            #Eqn 8 start\n",
        "            for w in range(len(features)):\n",
        "                PDu_wIc[t][c][w] = PDu_Du_*PDu_cIDu[t][c]*PDu_wIc_Du[w][c] + PDu_Dl_*PDu_cIDl[t][c]*PDu_wIc_Dl[w][c]\n",
        "            #Eqn 8 end\n",
        "    return PDu_cId[T]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f21a58f",
      "metadata": {
        "scrolled": true,
        "id": "0f21a58f"
      },
      "outputs": [],
      "source": [
        "X_train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5319330e",
      "metadata": {
        "id": "5319330e"
      },
      "source": [
        "### Algorithm 2 as given in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb39e06d",
      "metadata": {
        "id": "bb39e06d"
      },
      "outputs": [],
      "source": [
        "def Algo2():\n",
        "    kl_div = KL(p1, p2)\n",
        "    gamma = 0.042*(math.pow(kl_div, -2.276))\n",
        "    PDu_Dl_ = gamma/(1+gamma)\n",
        "    PDu_Du_ = 1 - PDu_Dl_\n",
        "    ans = [[0 for j in range(2)] for k in range(len(X_train_dataset))]\n",
        "    ans = Algo1()\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128ed7e9",
      "metadata": {
        "scrolled": true,
        "id": "128ed7e9"
      },
      "outputs": [],
      "source": [
        "pred = Algo2()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1cf95a",
      "metadata": {
        "collapsed": true,
        "id": "4f1cf95a"
      },
      "outputs": [],
      "source": [
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0be64b8",
      "metadata": {
        "id": "e0be64b8"
      },
      "outputs": [],
      "source": [
        "Y_pred_algo = []\n",
        "for i in range(len(pred)):\n",
        "    if(pred[i][0] > pred[i][1]):\n",
        "        Y_pred_algo.append(0)\n",
        "    else:\n",
        "        Y_pred_algo.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04e4a87",
      "metadata": {
        "id": "b04e4a87"
      },
      "outputs": [],
      "source": [
        "Y_test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c541b1f1",
      "metadata": {
        "id": "c541b1f1"
      },
      "outputs": [],
      "source": [
        "Y_pred_algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15c5659",
      "metadata": {
        "id": "d15c5659"
      },
      "outputs": [],
      "source": [
        "pred_count = 0\n",
        "for i in range(len(X_test_dataset)):\n",
        "    if(Y_test_dataset[i] == Y_pred_algo[i]):\n",
        "        pred_count += 1\n",
        "        \n",
        "print(pred_count/len(X_test_dataset))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}